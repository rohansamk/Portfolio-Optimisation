# Portfolio-Optimisation Project Overview

The primary goal of this project is to illustrate proficiency in handling diverse datasets, implementing advanced analytics techniques, and delivering actionable insights. This project explores the full data analysis pipeline, including:

Data Acquisition: Collecting data from various sources, including APIs and web scraping.
Data Wrangling: Cleaning, transforming, and preparing data for analysis.
Exploratory Data Analysis (EDA): Uncovering insights through visualizations and statistical summaries.
Data Analysis: Formulating and addressing research questions using advanced analytical techniques.
Data Persistence: Utilizing SQLite for storing and managing datasets effectively.
Final Reporting: Documenting findings in a clear, concise, and visual format.
Key Features

Dynamic Data Sources: Integration of datasets from APIs and web scraping to ensure up-to-date analyses.
Custom Visualizations: Includes pivot tables, cross-tabulations, and rich visualizations for better comprehension.
Advanced Analytics: Covers segmentation, trend analysis, and statistical modeling.
Reproducibility: A clean and structured workflow documented in a Jupyter Notebook for seamless replication.
SQLite Integration: Demonstrates the use of SQLite for data persistence and query optimization.
Technologies and Tools

Programming Language: Python
Libraries:
pandas
numpy
matplotlib
seaborn
sqlite3
Additional libraries as needed for specific tasks
Data Sources: Web APIs, scraped data, and static datasets
Database: SQLite
Notebook: Jupyter Notebook
